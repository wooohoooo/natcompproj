{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A 15 to  max 20 pages document. The report should include:\n",
    "\n",
    "• Specific goals of the work.\n",
    "\n",
    "• Review of related work.\n",
    "\n",
    "• Description of your testing procedures.\n",
    "\n",
    "• Description of the algorithms used. Ensure that the work is reproducible given your\n",
    "description.\n",
    "\n",
    "• Results. Ensure to perform a correct experimental analysis (e.g. repeated experiments,\n",
    "statistical analysis of results).\n",
    "\n",
    "• Conclusion. Is your line of work worth pursuing? What additional enhancements could be made?\n",
    "\n",
    "• A short description of the contribution and self-evaluation of each member of the team\n",
    "\n",
    "• URL address where we can find implementation of algorithms developed and used during the project: 1) source code, together with all data necessary to run the program and a short manual describing how to use/run the program, and 2) a sample run, screenshot, or other indication of system behaviour.\n",
    "\n",
    "Deadline: 20 June."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously there is a similarity between samples from a distribution of models and ensembles. This work aims to look at those in more detail and "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fully bayesian models start with a distribution over parameters (And a prior, but were neglecting that one for now.)\n",
    "From that distribution we can sample 'point estimate' models that are expected to be reasonably well fitted to the problem\n",
    "\n",
    "Ensembling does the inverse: It trains 'point estimate' models that are reasonably well fitted to the problem, and from there we can approximate the distribution that those would have been sampled from, e.g. by taking the mean and the variance over all our models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A note on priors:\n",
    "\n",
    "A prior is usually specified as a distribution that takes on a certain form that is defined by parameters. So usually, a prior is reported to be e.g. a normal distribution with sigma = 1 and mu = 0; This will then be the basis on which the bayesian modelling process is built upon.\n",
    "However, in our Enssemble, having all the 'draws' stemming from the same kind of distribution (e.g. all from a normal) is a special case rather than normality. One such special case would be as reported by [simple and scalable predictive uncertainty], where the authors report that they use the same Deep Neural Network Structure while only changing the initial weights by a random factor. However, usually, we might have a mix of differen lengths of RegressionTrees, Deep networks with different number of layers and different numbers of nodes in each layer.\n",
    "In the first case (same structure, different initialisations), our outcome is philosophically assuming that the TRUE/Best model is somewhere in the parameter space of the strucutre of the network, i.e. that a network exists that would be an optimal predictor for the given case, and this model is being approximated by bayesian statistics.\n",
    "In the other case (vastly different models), we do not make such an assumption - in this case the TRUE/best model could very well be a combination of the models that are put in the ensemble, or some interaction between them that helps the model be more expressive. \n",
    "Essentially, in the first case we specify exactly the FORM (not the values) of our prior, in the second case we THEROETICALLY leave the form of the knowledge free - it can be a normal distribution, or approximate one; but in the end the mapping from ipit to output might be mutli-[hilled] or simething similar.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a note on correlation:\n",
    "\n",
    "Assuming the previous is true, we run into a practical problem: If we assume the ensemble members to be independent draws from an unlimited space of models that enables us to approximate the optimal model - how do we guarantee that the members are uncorrelated and thus valid samples from that space? This problem has been known for long (see e.g [Fast decorrelated neural network ensembles with random weights]), and solutions have been proposed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plan:\n",
    "\n",
    "\n",
    "1. Describe why Ensembles and Draws from a distribution are similar.\n",
    "Work through the Math? \n",
    "Talk about how the it's a bit of a reverse case?\n",
    "\n",
    "2. Show that it is possible to obtain uncertainty from these ensembles, both from simple linear models and things like regressiontrees\n",
    "\n",
    "3. Analyse that uncertainty?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the uncertainty in the ensemble case can be reduced to a 'disagreement between ensemble members'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
